{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc823eb-a5da-4cc8-8827-f7932ee7adb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c5a328-2e4a-403c-bdda-d162fb9e1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "# import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd095571-2cc3-4a14-aa97-bfd5c669c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPUCHIN_FILE = os.path.join('data','Parsed_Capuchinbird_Clips','XC3776-3.wav')\n",
    "NOT_CAPUCHIN_FILE = os.path.join('data','Parsed_Not_Capuchinbird_Clips','afternoon-birds-song-in-forest-0.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6d459b-fcfa-4b34-8f44-c24c9a99f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPUCHIN_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1662833-f19e-46da-b04e-50da1a366268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "def load_wav_16k_mono(filename):\n",
    "    import numpy as np\n",
    "    import librosa\n",
    "\n",
    "    # Safely convert the input to string\n",
    "    if isinstance(filename, np.ndarray):\n",
    "        # If it's an array of bytes like [b'd:/path/to/file.wav']\n",
    "        filename = filename.tolist()\n",
    "        if isinstance(filename, list) and isinstance(filename[0], bytes):\n",
    "            filename = filename[0].decode(\"utf-8\")\n",
    "        elif isinstance(filename, list):\n",
    "            filename = ''.join([char.decode(\"utf-8\") if isinstance(char, bytes) else char for char in filename])\n",
    "    elif isinstance(filename, bytes):\n",
    "        filename = filename.decode(\"utf-8\")\n",
    "\n",
    "    print(\"Loading file:\", filename)  # Debugging line\n",
    "\n",
    "    wav, sr = librosa.load(filename, sr=16000, mono=True)\n",
    "    return np.array(wav, dtype=np.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8b06f9-241c-4f60-a867-a66b62c4b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a9f08f-7193-42f7-8db6-ac4b5763c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load waveforms\n",
    "wave = load_wav_16k_mono(CAPUCHIN_FILE)\n",
    "nwave = load_wav_16k_mono(NOT_CAPUCHIN_FILE)\n",
    "\n",
    "# Plot both waveforms\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(wave)\n",
    "plt.title(\"Capuchinbird Call\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(nwave)\n",
    "plt.title(\"Not Capuchinbird Call\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e24e30-70fa-4390-9fde-89947c9fea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "\n",
    "# 3.1 Define paths\n",
    "POS = os.path.join('data', 'Parsed_Capuchinbird_Clips')\n",
    "NEG = os.path.join('data', 'Parsed_Not_Capuchinbird_Clips')\n",
    "\n",
    "# Count files manually since `len(tf.data.Dataset)` doesn't work\n",
    "pos_files = glob.glob(POS + '/*.wav')\n",
    "neg_files = glob.glob(NEG + '/*.wav')\n",
    "\n",
    "# 3.2 Create TensorFlow Datasets of file paths\n",
    "pos_ds = tf.data.Dataset.from_tensor_slices(pos_files)\n",
    "neg_ds = tf.data.Dataset.from_tensor_slices(neg_files)\n",
    "\n",
    "# 3.3 Add labels and combine datasets\n",
    "positives = pos_ds.map(lambda x: (x, tf.constant(1.0)))  # Label 1 for capuchin\n",
    "negatives = neg_ds.map(lambda x: (x, tf.constant(0.0)))  # Label 0 for not capuchin\n",
    "\n",
    "data = positives.concatenate(negatives)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00fa424-84b2-4d3b-b6b4-dfe43d3df5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shuffle(buffer_size=1000).as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d88430-9797-4f60-a82c-a0511129049d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for file in os.listdir(os.path.join('data', 'Parsed_Capuchinbird_Clips')):\n",
    "    tensor_wave = load_wav_16k_mono(os.path.join('data', 'Parsed_Capuchinbird_Clips', file))\n",
    "    lengths.append(len(tensor_wave))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135a324-f41d-42ff-9960-0c8591bc8e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_mean(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253ae31f-711c-43e9-9df1-e583b302a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_min(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7cb9d7-8742-48b1-b448-65550c0dfacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7847eb19-d898-4a24-bfa8-53d88ff47c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(file_path, label): \n",
    "#     wav = load_wav_16k_mono(file_path)\n",
    "#     wav = wav[:48000]\n",
    "#     zero_padding = tf.zeros([48000] - tf.shape(wav), dtype=tf.float32)\n",
    "#     wav = tf.concat([wav, zero_padding], axis=0)\n",
    "\n",
    "#     # Convert to spectrogram\n",
    "#     spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)\n",
    "#     spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "#     # Add channel dimension\n",
    "#     spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
    "\n",
    "#     return spectrogram, label\n",
    "def preprocess(file_path, label):\n",
    "    wav = tf.numpy_function(load_wav_16k_mono, [file_path], tf.float32)\n",
    "    wav.set_shape([None])\n",
    "\n",
    "    # Pad or trim to 48000 samples\n",
    "    wav = wav[:48000]\n",
    "    paddings = tf.maximum(48000 - tf.shape(wav)[0], 0)\n",
    "    wav = tf.pad(wav, paddings=[[0, paddings]])\n",
    "\n",
    "    # Compute spectrogram\n",
    "    spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
    "\n",
    "    # Optional: explicitly set shape to help model input sanity check\n",
    "    spectrogram.set_shape([1491, 257, 1])\n",
    "\n",
    "    label = tf.cast(label, tf.float32)\n",
    "    return spectrogram, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686e2d8-24a9-4a22-84a0-2b8352f705cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample file from the positives dataset\n",
    "file_path, label = positives.shuffle(10000).as_numpy_iterator().next()\n",
    "\n",
    "# Convert byte string path to string if needed\n",
    "file_path = file_path.decode(\"utf-8\")\n",
    "\n",
    "# Apply preprocessing\n",
    "spectrogram, label = preprocess(file_path, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566e6cbf-456f-4b2f-9521-cd67b4d26284",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,20))\n",
    "plt.imshow(tf.transpose(spectrogram)[0])\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Frequency bins\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57270bc2-9515-407a-b455-1281ec713ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.map(preprocess)                  # Apply spectrogram preprocessing\n",
    "# data = data.cache()                          # Cache in memory for performance\n",
    "# data = data.shuffle(buffer_size=1000)        # Shuffle to randomize order\n",
    "# data = data.batch(16)                        # Batch the data (size = 16)\n",
    "# data = data.prefetch(8)                      # Prefetch 8 batches to improve pipeline speed\n",
    "data = data.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "data = data.cache()\n",
    "data = data.shuffle(1000)\n",
    "data = data.batch(4)\n",
    "data = data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2b57b2-0753-4f0a-97e9-00ab69dab0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.take(36)\n",
    "test = data.skip(36).take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a210a85-fb35-4b59-ac32-698be2ecd126",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples, labels = train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e904a615-59c4-425b-aa50-2414200d1efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9016e1ff-5ca0-441f-8cc2-a560bef0419b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d575dd6-aac5-4c3b-b0ad-c88a1c871036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ff95c-5337-4367-a157-f2c0b56a81f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten,MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',       # Watch validation loss\n",
    "    patience=3,               # Wait 3 epochs without improvement\n",
    "    restore_best_weights=True  # Restore weights from the best epoch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9921dfc8-a165-47cb-bcab-21a77e465353",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(1491, 257, 1)))\n",
    "model.add(MaxPooling2D((4, 4)))  # ✅ Reduce spatial size\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "  # ✅ Further reduction\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ff086-a954-402a-bd6a-6c4776f7f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\"Adam\",loss=\"BinaryCrossentropy\",metrics=[tf.keras.metrics.Recall(),tf.keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35978eeb-a860-449a-b565-7898df2ea7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df08f4-4f98-48b7-8f57-fa1e099357b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train,epochs=10,validation_data=test,verbose=1,callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8907ad6-be38-4fc0-9f6f-ce634e299fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(hist.history.keys())\n",
    "# Plot Loss\n",
    "plt.title('Loss')\n",
    "plt.plot(hist.history['loss'], 'r', label='Train Loss')\n",
    "plt.plot(hist.history['val_loss'], 'b', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Precision\n",
    "plt.title('Precision')\n",
    "plt.plot(hist.history['precision'], 'r', label='Train Precision')\n",
    "plt.plot(hist.history['val_precision'], 'b', label='Validation Precision')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Recall\n",
    "plt.title('Recall')\n",
    "plt.plot(hist.history['recall'], 'r', label='Train Recall')\n",
    "plt.plot(hist.history['val_recall'], 'b', label='Validation Recall')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b847443-4307-4af6-895a-f982b1412306",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = test.as_numpy_iterator().next()\n",
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4610e95-e3f1-4930-8f8c-8258bd2583e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = [1 if prediction > 0.5 else 0 for prediction in yhat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76488c3d-f62f-437c-8eb6-fe78f5e038ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573674f3-b66a-482f-8dc5-eba2a0a2241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_mp3_16k_mono(filename):\n",
    "#     \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
    "#     res = tfio.audio.AudioIOTensor(filename)\n",
    "#     # Convert to tensor and combine channels \n",
    "#     tensor = res.to_tensor()\n",
    "#     tensor = tf.math.reduce_sum(tensor, axis=1) / 2 \n",
    "#     # Extract sample rate and cast\n",
    "#     sample_rate = res.rate\n",
    "#     sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "#     # Resample to 16 kHz\n",
    "#     wav = tfio.audio.resample(tensor, rate_in=sample_rate, rate_out=16000)\n",
    "#     return wav\n",
    "# mp3 = os.path.join('data', 'Forest Recordings', 'recording_00.mp3')\n",
    "# wav = load_mp3_16k_mono(mp3)\n",
    "# audio_slices = tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length=48000, sequence_stride=48000, batch_size=1)\n",
    "# samples, index = audio_slices.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da3787-ca17-47ba-8936-50369d45641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def load_mp3_librosa(filepath, sr=16000):\n",
    "    # Load MP3 using librosa, convert to mono, resample to 16kHz\n",
    "    wav, _ = librosa.load(filepath, sr=sr, mono=True)\n",
    "    return wav\n",
    "\n",
    "def create_windows(audio, window_size=48000, stride=48000):\n",
    "    windows = []\n",
    "    for i in range(0, len(audio) - window_size + 1, stride):\n",
    "        window = audio[i:i+window_size]\n",
    "        windows.append(window)\n",
    "    return np.array(windows)\n",
    "\n",
    "def compute_spectrogram_librosa(audio_window, n_fft=320, hop_length=32):\n",
    "    # STFT returns complex values\n",
    "    stft = librosa.stft(audio_window, n_fft=n_fft, hop_length=hop_length)\n",
    "    spectrogram = np.abs(stft)\n",
    "    # Add channel dimension to match shape (time, freq, 1)\n",
    "    spectrogram = np.expand_dims(spectrogram.T, axis=-1)  # shape: (time, freq_bins, 1)\n",
    "    return spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaf859d-41af-4c80-9fa7-d777c78bfc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mp3_librosa(filepath):\n",
    "    wav = load_mp3_librosa(filepath)\n",
    "    windows = create_windows(wav, window_size=48000, stride=48000)\n",
    "    \n",
    "    spectrograms = []\n",
    "    for window in windows:\n",
    "        spec = compute_spectrogram_librosa(window)\n",
    "        spectrograms.append(spec)\n",
    "    \n",
    "    return np.array(spectrograms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ad917-c3bb-47ef-857c-a42d9238dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/Forest Recordings/recording_00.mp3'\n",
    "spectrograms = preprocess_mp3_librosa(path)\n",
    "print(spectrograms.shape)  # should be like (4, 1491, 257, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee09cde-53a5-4b65-9ca3-4ddaceda9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_to_spectrogram(wav_slices, n_fft=320, hop_length=32):\n",
    "    spectrograms = []\n",
    "    for slice in wav_slices:\n",
    "        stft = librosa.stft(slice, n_fft=n_fft, hop_length=hop_length)\n",
    "        spect = np.abs(stft)\n",
    "        spect = np.expand_dims(spect.T, axis=-1)  # shape: (time, freq, 1)\n",
    "        spectrograms.append(spect)\n",
    "    return np.array(spectrograms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcce66a-14a6-4f2e-b886-0245cb4fd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24794d-36cb-469a-801d-1fadfce1e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1728d999-b4e5-455a-b593-ccc04d5130c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e61010-17ef-48f5-b5b9-638b319b1c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mp3(sample, label):\n",
    "    sample = sample[0]  # shape: [16000]\n",
    "    zero_padding = tf.zeros([48000] - tf.shape(sample), dtype=tf.float32)\n",
    "    wav = tf.concat([sample, zero_padding], axis=0)\n",
    "    \n",
    "    spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
    "    \n",
    "    return spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d9c3c3-8dd0-45e8-b587-64a77a9ab38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_slices = (\n",
    "    tf.keras.utils.timeseries_dataset_from_array(wav, wav, sequence_length=16000, sequence_stride=16000, batch_size=1)\n",
    "    .map(preprocess_mp3)\n",
    "    .batch(64)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059af96e-a957-4df3-8eb4-6f2323b51096",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(audio_slices)\n",
    "yhat = [1 if prediction > 0.5 else 0 for prediction in yhat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491dd785-2783-4034-b42e-0dbaffa4e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "yhat = [key for key, group in groupby(yhat)]\n",
    "calls = tf.math.reduce_sum(yhat).numpy()\n",
    "calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b2cb3-1f2f-4298-9c32-76b3852968e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for file in os.listdir(os.path.join('data', 'Forest Recordings')):\n",
    "    FILEPATH = os.path.join('data', 'Forest Recordings', file)\n",
    "    \n",
    "    wav = load_mp3_librosa(FILEPATH)\n",
    "    \n",
    "    audio_slices = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        wav, wav,\n",
    "        sequence_length=48000,\n",
    "        sequence_stride=48000,\n",
    "        batch_size=1\n",
    "    )\n",
    "    \n",
    "    audio_slices = audio_slices.map(preprocess_mp3)\n",
    "    audio_slices = audio_slices.batch(64)\n",
    "    \n",
    "    yhat = model.predict(audio_slices)\n",
    "    results[file] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0497634-4170-4938-b45f-3b9e7513deb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34644569-a434-46c2-a71b-a1a82ebba070",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_preds = {}\n",
    "for file, logits in results.items():\n",
    "    class_preds[file] = [1 if prediction > 0.99 else 0 for prediction in logits]\n",
    "class_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2486d422-5e6a-43a0-a9c9-201ad5b5859d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "postprocessed = {}\n",
    "for file, scores in class_preds.items():\n",
    "    postprocessed[file] = tf.math.reduce_sum([key for key, group in groupby(scores)]).numpy()\n",
    "postprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e4c2d1-7213-4e71-81b3-95a987d9e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import groupby\n",
    "\n",
    "def load_mp3_16k_mono_librosa(filename):\n",
    "    wav, _ = librosa.load(filename, sr=16000, mono=True)\n",
    "    return tf.convert_to_tensor(wav, dtype=tf.float32)\n",
    "\n",
    "def preprocess_mp3(sample, index=None):\n",
    "    sample = sample[0]\n",
    "    zero_padding = tf.zeros([48000] - tf.shape(sample), dtype=tf.float32)\n",
    "    wav = tf.concat([zero_padding, sample], 0)\n",
    "    spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
    "    return spectrogram\n",
    "\n",
    "def classify_audio_file(filepath, model, threshold=0.99):\n",
    "    # Step 1: Load MP3\n",
    "    wav = load_mp3_16k_mono_librosa(filepath)\n",
    "\n",
    "    # Step 2: Slice into 3s chunks\n",
    "    audio_slices = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        wav, wav,\n",
    "        sequence_length=48000,\n",
    "        sequence_stride=48000,\n",
    "        batch_size=1\n",
    "    )\n",
    "\n",
    "    # Step 3: Preprocess and batch\n",
    "    audio_slices = audio_slices.map(preprocess_mp3)\n",
    "    audio_slices = audio_slices.batch(64)\n",
    "\n",
    "    # Step 4: Predict\n",
    "    predictions = model.predict(audio_slices)\n",
    "\n",
    "    # Step 5: Convert to class labels\n",
    "    binary_preds = [1 if pred > threshold else 0 for pred in predictions]\n",
    "\n",
    "    # Step 6: Count distinct detection groups\n",
    "    detection_count = tf.math.reduce_sum([k for k, _ in groupby(binary_preds)]).numpy()\n",
    "\n",
    "    return {\n",
    "        \"file\": os.path.basename(filepath),\n",
    "        \"predictions\": binary_preds,\n",
    "        \"detection_count\": detection_count\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a02eb-6510-41de-99f6-9f29d5e9e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_file = 'data/Forest Recordings/recording_08.mp3'\n",
    "output = classify_audio_file(user_file, model)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff00f045-4b4b-49c1-9fb0-2e51fbe0f8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861d052a-e737-46ce-b147-747ac93e2020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
